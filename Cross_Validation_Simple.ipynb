{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cross-Validation Simple.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/todnewman/coe_training/blob/master/Cross_Validation_Simple.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRbiJoiJm8qO"
      },
      "source": [
        "# Simple Cross-Validation Example\n",
        "Code Developed by: W. Tod Newman\n",
        "\n",
        "### Overview\n",
        "Here we show how to use grid search to identify the best hyperparameters for a Support Vector Machine.  We evaluate linear vs. non-linear SVM, kernel coefficient (gamma) values, and loss penalty (C) values.  The dataset we're evaluating is the MNIST handwritten digits dataset.\n",
        "\n",
        "### Note:\n",
        "* This is a very simple example for the reason that grid search takes a very long time.  In class, none of us have this kind of patience.\n",
        "* However, grid search is still valuable and DOESN'T have to be run every time you train a model.  Often times running it early in a prototyping effort is all you need.\n",
        "\n",
        "### Learning Objectives\n",
        "Show how grid search can be an effective way to evaluate hyperparameters of small models.\n",
        "\n",
        "Opportunities to extend this learning include evaluating the contribution of the penalty factors and potential approaches to force overfitting.  Applying this tool to different datasets that are less predictable will also reveal larger differences between the hyperparameter sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ESfS8sVk3Yg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17701fb5-fa5f-43f3-8f03-827492870a4c"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from sklearn import datasets\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import SVC\n",
        "from keras import backend as K\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# Loading the Digits dataset\n",
        "digits = datasets.load_digits()\n",
        "\n",
        "# To apply an classifier on this data, we need to flatten the image, to\n",
        "# turn the data in a (samples, feature) matrix:\n",
        "n_samples = len(digits.images)\n",
        "X = digits.images.reshape((n_samples, -1))\n",
        "y = digits.target\n",
        "\n",
        "\n",
        "# Split the dataset in two equal parts\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=0)\n",
        "    \n",
        "\"\"\"\n",
        "Create an Array of Dictionaries to use for our Cross-Validation\n",
        "\n",
        "PARAMETERS:\n",
        "*  C is the regularization parameter for the SVC classifier \n",
        "*  kernel specifies the kernel type.  There are a few options but we'll experiment\n",
        "      with the radial basis function kernel (rbf) and the linear kernel.\n",
        "*  gamma is a specific kernel coefficient for rbf that represents the radius of \n",
        "      influence for any datapoint.\n",
        "\"\"\"\n",
        "\n",
        "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4, 1e-2, 2e-3],\n",
        "                     'C': [1, 10, 100, 1000]},\n",
        "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
        "\n",
        "scores = ['precision', 'recall']\n",
        "\n",
        "for score in scores:\n",
        "    print(\"***********************************************************\")\n",
        "    print(\"******** Tuning hyper-parameters for %s ************\" % score) \n",
        "    print(\"***********************************************************\")\n",
        "    print()\n",
        "    \n",
        "    #\n",
        "    # The grid search provided by GridSearchCV exhaustively generates \n",
        "    # candidates from a grid of parameter values specified with the \n",
        "    # param_grid parameter (in this case, our Array of Dictionaries, tuned_parameters). \n",
        "    #\n",
        "    clf = GridSearchCV(SVC(), tuned_parameters, cv=5,\n",
        "                       scoring='%s_macro' % score)\n",
        "    \n",
        "    print(X_train.shape, y_train.shape)\n",
        "    clf.fit(X_train, y_train) # FIT our grid_search model\n",
        "\n",
        "    print(\"***********************************************************\")\n",
        "    print(\"Best parameters set found on development set:\")\n",
        "    print()\n",
        "    print(clf.best_params_)\n",
        "    print()\n",
        "    print(\"Grid scores on development set:\")\n",
        "    print()\n",
        "    means = clf.cv_results_['mean_test_score']\n",
        "    stds = clf.cv_results_['std_test_score']\n",
        "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
        "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
        "              % (mean, std * 2, params))\n",
        "    print()\n",
        "    print(\"***********************************************************\")\n",
        "    print(\"Detailed classification report:\")\n",
        "    print()\n",
        "    print(\"The model is trained on the full development set.\")\n",
        "    print(\"The scores are computed on the full evaluation set.\")\n",
        "    print()\n",
        "    y_true, y_pred = y_test, clf.predict(X_test)\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    print()\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***********************************************************\n",
            "******** Tuning hyper-parameters for precision ************\n",
            "***********************************************************\n",
            "\n",
            "(1257, 64) (1257,)\n",
            "***********************************************************\n",
            "Best parameters set found on development set:\n",
            "\n",
            "{'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "\n",
            "Grid scores on development set:\n",
            "\n",
            "0.989 (+/-0.006) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "0.970 (+/-0.013) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "0.933 (+/-0.009) for {'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
            "0.987 (+/-0.010) for {'C': 1, 'gamma': 0.002, 'kernel': 'rbf'}\n",
            "0.991 (+/-0.006) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "0.987 (+/-0.013) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "0.936 (+/-0.011) for {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
            "0.988 (+/-0.011) for {'C': 10, 'gamma': 0.002, 'kernel': 'rbf'}\n",
            "0.991 (+/-0.006) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "0.988 (+/-0.013) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "0.936 (+/-0.011) for {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n",
            "0.988 (+/-0.011) for {'C': 100, 'gamma': 0.002, 'kernel': 'rbf'}\n",
            "0.991 (+/-0.006) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "0.988 (+/-0.013) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "0.936 (+/-0.011) for {'C': 1000, 'gamma': 0.01, 'kernel': 'rbf'}\n",
            "0.988 (+/-0.011) for {'C': 1000, 'gamma': 0.002, 'kernel': 'rbf'}\n",
            "0.984 (+/-0.014) for {'C': 1, 'kernel': 'linear'}\n",
            "0.984 (+/-0.014) for {'C': 10, 'kernel': 'linear'}\n",
            "0.984 (+/-0.014) for {'C': 100, 'kernel': 'linear'}\n",
            "0.984 (+/-0.014) for {'C': 1000, 'kernel': 'linear'}\n",
            "\n",
            "***********************************************************\n",
            "Detailed classification report:\n",
            "\n",
            "The model is trained on the full development set.\n",
            "The scores are computed on the full evaluation set.\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        45\n",
            "           1       0.98      1.00      0.99        52\n",
            "           2       1.00      0.98      0.99        53\n",
            "           3       1.00      1.00      1.00        54\n",
            "           4       1.00      1.00      1.00        48\n",
            "           5       0.98      0.96      0.97        57\n",
            "           6       0.98      1.00      0.99        60\n",
            "           7       0.98      1.00      0.99        53\n",
            "           8       1.00      0.98      0.99        61\n",
            "           9       0.98      0.98      0.98        57\n",
            "\n",
            "    accuracy                           0.99       540\n",
            "   macro avg       0.99      0.99      0.99       540\n",
            "weighted avg       0.99      0.99      0.99       540\n",
            "\n",
            "\n",
            "***********************************************************\n",
            "******** Tuning hyper-parameters for recall ************\n",
            "***********************************************************\n",
            "\n",
            "(1257, 64) (1257,)\n",
            "***********************************************************\n",
            "Best parameters set found on development set:\n",
            "\n",
            "{'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "\n",
            "Grid scores on development set:\n",
            "\n",
            "0.989 (+/-0.006) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "0.969 (+/-0.012) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "0.820 (+/-0.060) for {'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
            "0.986 (+/-0.011) for {'C': 1, 'gamma': 0.002, 'kernel': 'rbf'}\n",
            "0.991 (+/-0.006) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "0.986 (+/-0.013) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "0.834 (+/-0.067) for {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
            "0.988 (+/-0.012) for {'C': 10, 'gamma': 0.002, 'kernel': 'rbf'}\n",
            "0.991 (+/-0.006) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "0.987 (+/-0.014) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "0.834 (+/-0.067) for {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n",
            "0.988 (+/-0.012) for {'C': 100, 'gamma': 0.002, 'kernel': 'rbf'}\n",
            "0.991 (+/-0.006) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "0.987 (+/-0.014) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "0.834 (+/-0.067) for {'C': 1000, 'gamma': 0.01, 'kernel': 'rbf'}\n",
            "0.988 (+/-0.012) for {'C': 1000, 'gamma': 0.002, 'kernel': 'rbf'}\n",
            "0.983 (+/-0.014) for {'C': 1, 'kernel': 'linear'}\n",
            "0.983 (+/-0.014) for {'C': 10, 'kernel': 'linear'}\n",
            "0.983 (+/-0.014) for {'C': 100, 'kernel': 'linear'}\n",
            "0.983 (+/-0.014) for {'C': 1000, 'kernel': 'linear'}\n",
            "\n",
            "***********************************************************\n",
            "Detailed classification report:\n",
            "\n",
            "The model is trained on the full development set.\n",
            "The scores are computed on the full evaluation set.\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        45\n",
            "           1       0.98      1.00      0.99        52\n",
            "           2       1.00      0.98      0.99        53\n",
            "           3       1.00      1.00      1.00        54\n",
            "           4       1.00      1.00      1.00        48\n",
            "           5       0.98      0.96      0.97        57\n",
            "           6       0.98      1.00      0.99        60\n",
            "           7       0.98      1.00      0.99        53\n",
            "           8       1.00      0.98      0.99        61\n",
            "           9       0.98      0.98      0.98        57\n",
            "\n",
            "    accuracy                           0.99       540\n",
            "   macro avg       0.99      0.99      0.99       540\n",
            "weighted avg       0.99      0.99      0.99       540\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmcdQscSjtvL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}